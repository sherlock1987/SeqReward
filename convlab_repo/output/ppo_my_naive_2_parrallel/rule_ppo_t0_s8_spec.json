{
  "agent": [
    {
      "name": "DialogAgent",
      "dst": {
        "name": "RuleDST"
      },
      "state_encoder": {
        "name": "MultiWozStateEncoder"
      },
      "action_decoder": {
        "name": "MultiWozVocabActionDecoder"
      },
      "algorithm": {
        "name": "PPO",
        "action_pdtype": "default",
        "action_policy": "default",
        "explore_var_spec": null,
        "gamma": 0.99,
        "lam": 0.9500000000000001,
        "reward_type": "OFFGAN",
        "load_pretrain_policy": true,
        "reward_buffer_size": 10,
        "disc_training_times": 10,
        "disc_training_freq": 4,
        "clip_eps_spec": {
          "name": "linear_decay",
          "start_val": 0.2,
          "end_val": 0.2,
          "start_step": 1000,
          "end_step": 5000000
        },
        "entropy_coef_spec": {
          "name": "linear_decay",
          "start_val": 0.001,
          "end_val": 0.001,
          "start_step": 1000,
          "end_step": 500000
        },
        "val_loss_coef": 1.0,
        "training_frequency": 500,
        "training_epoch": 4
      },
      "memory": {
        "name": "OnPolicyBatchReplay"
      },
      "net": {
        "type": "MLPNet",
        "shared": false,
        "hid_layers": [
          100
        ],
        "hid_layers_activation": "relu",
        "clip_grad_val": 10.0,
        "use_same_optim": false,
        "actor_optim_spec": {
          "name": "RMSprop",
          "lr": 0.0001
        },
        "critic_optim_spec": {
          "name": "Adam",
          "lr": 0.001
        },
        "lr_scheduler_spec": {
          "name": "StepLR",
          "step_size": 2000,
          "gamma": 0.999
        },
        "gpu": false
      }
    }
  ],
  "env": [
    {
      "name": "multiwoz",
      "action_dim": 300,
      "observation_dim": 392,
      "max_t": 40,
      "max_frame": 500000,
      "user_policy": {
        "name": "UserPolicyAgendaMultiWoz"
      },
      "sys_policy": {
        "name": "RuleBasedMultiwozBot"
      }
    }
  ],
  "meta": {
    "distributed": false,
    "num_eval": 100,
    "eval_frequency": 1000,
    "max_tick_unit": "total_t",
    "max_trial": 1,
    "max_session": 10,
    "resources": {
      "num_cpus": 1,
      "num_gpus": 0
    },
    "experiment": 0,
    "trial": 0,
    "session": 8,
    "cuda_offset": 0,
    "experiment_ts": "2020_10_28_175515",
    "prepath": "output/rule_ppo_2020_10_28_175515/rule_ppo_t0_s8",
    "ckpt": null,
    "git_sha": "f2f61c91cc9ba78445ab8b94beaf75b435b1023b",
    "random_seed": 1603886932,
    "eval_model_prepath": null,
    "graph_prepath": "output/rule_ppo_2020_10_28_175515/graph/rule_ppo_t0_s8",
    "info_prepath": "output/rule_ppo_2020_10_28_175515/info/rule_ppo_t0_s8",
    "log_prepath": "output/rule_ppo_2020_10_28_175515/log/rule_ppo_t0_s8",
    "model_prepath": "output/rule_ppo_2020_10_28_175515/model/rule_ppo_t0_s8"
  },
  "name": "rule_ppo"
}